We have 2 versions of web based (client side) LLM chat for ollama.com server. One version is made by claude.ai and the other by me. Both include scripts that fetch the list of models to choose from.

In Windows, you must set environment variables for Ollama (2 settings, see img) or you will get CORS errors.

![image](https://github.com/user-attachments/assets/8093de72-b1f6-4946-bc45-470c9af28412)

